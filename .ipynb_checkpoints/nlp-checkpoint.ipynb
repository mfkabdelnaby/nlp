{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Handling-Text-in-Python\" data-toc-modified-id=\"Handling-Text-in-Python-1\">Handling Text in Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Number-of-Characters-and-Words\" data-toc-modified-id=\"Number-of-Characters-and-Words-1.1\">Number of Characters and Words</a></span></li><li><span><a href=\"#Finding-Specific-Words\" data-toc-modified-id=\"Finding-Specific-Words-1.2\">Finding Specific Words</a></span></li><li><span><a href=\"#Finding-Unique-Words:-Using-set()\" data-toc-modified-id=\"Finding-Unique-Words:-Using-set()-1.3\">Finding Unique Words: Using set()</a></span></li></ul></li><li><span><a href=\"#Detecting-Spam-SMS-Messages\" data-toc-modified-id=\"Detecting-Spam-SMS-Messages-2\">Detecting Spam SMS Messages</a></span></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-3\">Exploratory Data Analysis</a></span></li><li><span><a href=\"#Data-Visualization\" data-toc-modified-id=\"Data-Visualization-4\">Data Visualization</a></span></li><li><span><a href=\"#Text-Processing\" data-toc-modified-id=\"Text-Processing-5\">Text Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-Punctuation\" data-toc-modified-id=\"Remove-Punctuation-5.1\">Remove Punctuation</a></span></li><li><span><a href=\"#Remove-Stopwords\" data-toc-modified-id=\"Remove-Stopwords-5.2\">Remove Stopwords</a></span></li></ul></li><li><span><a href=\"#Vectorization\" data-toc-modified-id=\"Vectorization-6\">Vectorization</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-7\">TF-IDF</a></span></li><li><span><a href=\"#Training-the-Model\" data-toc-modified-id=\"Training-the-Model-8\">Training the Model</a></span></li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-9\">Model Evaluation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Tasks with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get neccessary nltk packages\n",
    "nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Wall Street Journal>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one sentence from text7\n",
    "sent7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many words in sent7? (number of tokens?)\n",
    "len(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100676"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many tokens in the whole text7?\n",
    "len(text7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many unique tokens in text7?\n",
    "len(set(text7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neat',\n",
       " 'heads',\n",
       " 'Sasaki',\n",
       " 'can',\n",
       " 'vintages',\n",
       " 'objectionable',\n",
       " '12-member',\n",
       " 'producers',\n",
       " 'packages',\n",
       " 'identity-management']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first 10 unique tokens?\n",
    "list(set(text7))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of words\n",
    "\n",
    "* **`FreqDist`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = FreqDist(text7)\n",
    "len(dist)\n",
    "#the number of unique tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the **`.keys()`** of the frequency distribution to get the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre', 'Vinken', ',', '61', 'years', 'old', 'will', 'join', 'the', 'board']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab1 = dist.keys()\n",
    "# In Python 3 dict.keys() returns an iterable view instead of a list\n",
    "list(vocab1)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times did the word `four` appear in text7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist['four']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist.items()\n",
    "\n",
    "# returns (token,freqeuncy) dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the most 10 frequent (unique) words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4885),\n",
       " ('the', 4045),\n",
       " ('.', 3828),\n",
       " ('of', 2319),\n",
       " ('to', 2164),\n",
       " ('a', 1878),\n",
       " ('in', 1572),\n",
       " ('and', 1511),\n",
       " ('*-1', 1123),\n",
       " ('0', 1099)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = sorted(dist.items(), key=lambda x: x[1], reverse = True)    \n",
    "order[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words are more than 5 letters AND appeared more at least 100 times in the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion',\n",
       " 'company',\n",
       " 'president',\n",
       " 'because',\n",
       " 'market',\n",
       " 'million',\n",
       " 'shares',\n",
       " 'trading',\n",
       " 'program']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqwords = [w for w in vocab1 if len(w) > 5 and dist[w] > 100]\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "* **`nltk.word_tokenize(text)`**\n",
    "* **`nltk.sent_tokenize(text)`** sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children', \"shouldn't\", 'drink', 'a', 'sugary', 'drink', 'before', 'bed.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "text11.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Children',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'drink',\n",
       " 'a',\n",
       " 'sugary',\n",
       " 'drink',\n",
       " 'before',\n",
       " 'bed',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice `Shouldn't` is two words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text12 = \"This is the first sentence. A gallon of milk in the U.S. costs $2.99. Is this the third sentence? Yes, it is!\"\n",
    "sentences = nltk.sent_tokenize(text12)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first sentence.',\n",
       " 'A gallon of milk in the U.S. costs $2.99.',\n",
       " 'Is this the third sentence?',\n",
       " 'Yes, it is!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization and Stemming\n",
    "\n",
    "**Normalizing**\n",
    "* **`lower()`** un-capitalize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'listed', 'lists', 'listing', 'listings']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ')\n",
    "words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "Getting the roots of the words\n",
    "\n",
    "* **`nltk.PorterStemmer()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'rights',\n",
       " 'of']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print first 20 words of UDHR corpus\n",
    "udhr = nltk.corpus.udhr.words('English-Latin1')\n",
    "udhr[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do Stemming here, you will find that not all tokens are meaningful words. Therefore, you can use **Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['univers',\n",
       " 'declar',\n",
       " 'of',\n",
       " 'human',\n",
       " 'right',\n",
       " 'preambl',\n",
       " 'wherea',\n",
       " 'recognit',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inher',\n",
       " 'digniti',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalien',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in udhr[:20]] # Still Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`nltk.WordNetLemmatizer()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Universal',\n",
       " 'Declaration',\n",
       " 'of',\n",
       " 'Human',\n",
       " 'Rights',\n",
       " 'Preamble',\n",
       " 'Whereas',\n",
       " 'recognition',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inherent',\n",
       " 'dignity',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'equal',\n",
       " 'and',\n",
       " 'inalienable',\n",
       " 'right',\n",
       " 'of']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(t) for t in udhr[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuation using **`string`** library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "mess = 'Sample message! Notice: it has punctuation.'\n",
    "\n",
    "# Check characters to see if they are in punctuation\n",
    "nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "# Join the characters again to form the string.\n",
    "nopunc = ''.join(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords\n",
    "Now remove *stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')[0:10] # Show some stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_mess = [word for word in nopunc.split() \n",
    "              if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "clean_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine above code and write a function that removes **punctuation** and **stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punc_stopwords(text):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() \n",
    "            if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced NLP Tasks with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) tagging\n",
    "\n",
    "| Tag | World Class | Tag | World Class |\n",
    "|:---:|:-----------:|:---:|:-----------:|\n",
    "|  CC | Conjunction |  NN |     Noun    |\n",
    "|  CD |   Cardinal  | POS |  Possessive |\n",
    "|  DT |  Determiner | PRP |   Pronoun   |\n",
    "|  IN | Preposition |  RB |    Adverb   |\n",
    "|  JJ |  Adjective  | SYM |    Symbol   |\n",
    "|  MD |    Modal    |  VB |     Verb    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('MD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`nltk.pos_tag`** to get the tag of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text11 = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "\n",
    "text13 = nltk.word_tokenize(text11)\n",
    "nltk.pos_tag(text13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ambiguity in POS Tagging**\n",
    "\n",
    "In some English sentences you cannot tell the meaning of the them because they can be interpreted in different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Visiting', 'VBG'),\n",
       " ('aunts', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('nuisance', 'NN')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text14 = nltk.word_tokenize(\"Visiting aunts can be a nuisance\")\n",
    "nltk.pos_tag(text14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'), ('old', 'JJ'), ('man', 'NN'), ('the', 'DT'), ('boat', 'NN')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text18 = nltk.word_tokenize(\"The old man the boat\")\n",
    "nltk.pos_tag(text18)\n",
    "\n",
    "#you will get man as a noun which is not true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Colorless', 'NNP'),\n",
       " ('green', 'JJ'),\n",
       " ('ideas', 'NNS'),\n",
       " ('sleep', 'VBP'),\n",
       " ('furiously', 'RB')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text19 = nltk.word_tokenize(\"Colorless green ideas sleep furiously\")\n",
    "nltk.pos_tag(text19)\n",
    "\n",
    "#well-formed sentences but meaningless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## Detecting Spam SMS Messages\n",
    "\n",
    "The dataset we are using contains a collection of more than 5 thousand SMS phone messages. We’ll train a machine learning model to learn to discriminate between ham/spam messages automatically. Then, with a trained model, we’ll be able to classify arbitrary unlabeled messages as ham or spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the messages, and use **`.rstrip()`** to get rid of any extra spaces before or after the entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [line.rstrip() for line in \n",
    "            open('SMSSpamCollection')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print first 10 messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message_no, message in enumerate(messages[:10]):\n",
    "    print(message_no, message)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell from the output above that the first is *tab* separated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv('SMSSpamCollection',\n",
    "                       sep='\\t',names=[\"label\", \"message\"])\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check messages length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['length'] = messages['message'].apply(len)\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "messages['length'].plot(kind='hist', bins=50)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we see that some messages go up to 1000ish of length, which is so strange for a text message to be that long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_sms = messages['length'] > 200\n",
    "messages[long_sms].message.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on the message that has 910 characters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length'] == 910].message.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out if the message length is a feacture that distinguishes between ham and spam messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length', by='label', bins=50,figsize=(12,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through just basic EDA we’ve been able to discover a trend that spam messages tend to have more characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuation using **`string`** library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "mess = 'Sample message! Notice: it has punctuation.'\n",
    "\n",
    "# Check characters to see if they are in punctuation\n",
    "nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "# Join the characters again to form the string.\n",
    "nopunc = ''.join(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords\n",
    "Now remove *stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')[0:10] # Show some stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_mess = [word for word in nopunc.split() \n",
    "              if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "clean_mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine above code and write a function that removes **punctuation** and **stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() \n",
    "            if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process of transferring text into a list of meaningful words is called **tokenization**\n",
    "\n",
    "Let's try a sample before and after tekonization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(messages['message'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
      "1                       [Ok, lar, Joking, wif, u, oni]\n",
      "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
      "3        [U, dun, say, early, hor, U, c, already, say]\n",
      "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
      "Name: message, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(messages['message'].head(5).apply(text_process))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "We need to transform the bag-of-words created above into a form that sklean can work with. The steps to perform that are as follows:\n",
    "\n",
    "1. Count how many times does a word occur in each message (Known as term frequency)\n",
    "2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n",
    "\n",
    "This results in a matrix in which the rows are the words' counts (1 row per word), and the columns are the count of the presence of the word in each message (1 column per message)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the **analyzer** to be our own previously defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the vecotrization with one message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "message4 = messages['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that there are seven unique words in message number 4 (after removing common stop words). Two of them appear twice, the rest only once. Let's go ahead and check and confirm which ones appear twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[4068])\n",
    "print(bow_transformer.get_feature_names()[9554])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the entire bag-of-words messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (5572, 11425)\n",
      "Amount of Non-Zero occurences:  50548\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])\n",
    "messages_bow = bow_transformer.transform(messages['message'])\n",
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / \n",
    "            (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(round(sparsity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "TF-IDF stands for **term frequency-inverse document frequency**, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document’s relevance given a user query.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "**Term Frequency (TF):** Relative frequency of a term in a document\n",
    " = term instances / total terms\n",
    "\n",
    "**Inverse Document Frequency (IDF):** Relative count of documents containing ther term\n",
    " = log_e(Total number of documents / Number of documents with term in it)\n",
    "\n",
    "**Overall importance of a term =** TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works with message4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the IDF (inverse document frequency) of the word \"u\" and of word \"university\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2800524267409408\n",
      "8.527076498901426\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])\n",
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform the entire bag-of-words corpus into TF-IDF corpus at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Training will be done with **Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to predict our message4 to verify that the model is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ham\n",
      "expected: ham\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detect_model.predict(tfidf4)[0])\n",
    "print('expected:', messages.label[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detect_model.predict(messages_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      4825\n",
      "        spam       1.00      0.85      0.92       747\n",
      "\n",
      "    accuracy                           0.98      5572\n",
      "   macro avg       0.99      0.92      0.95      5572\n",
      "weighted avg       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(messages['label'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better evalaution, it is better to split the data into training and test sets .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
